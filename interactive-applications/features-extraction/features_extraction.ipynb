{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the useful modules of several libraries.\n",
    "from khiva.features import *\n",
    "from khiva.distances import *\n",
    "from khiva.library import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares the jupyter-notebook environment.\n",
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the local ids for each local.\n",
    "file_names = []\n",
    "\n",
    "for name in all_sites[\"SITE_ID\"].values:\n",
    "    file_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the metadata\n",
    "all_sites = pd.read_csv(\"../../energy/data/data-enerNoc/all-data/meta/all_sites.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KHIVABackend.KHIVA_BACKEND_CPU\n"
     ]
    }
   ],
   "source": [
    "# Prints the backend used. CPU, CUDA and OPENCL are available for khiva.\n",
    "print(get_backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load 100 time series. 1,666,600 data points in total.\n",
    "arr_tmp  = Array(np.load(\"../../energy/electric-consumption-rate-python/time-series-redimension-applied.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction.\n",
    "# Calculates several features of each time series in order to create the features matrix. \n",
    "start = time.time()\n",
    "features = np.stack([abs_energy(arr_tmp).to_numpy(),\n",
    "                    absolute_sum_of_changes(arr_tmp).to_numpy(),\n",
    "                    count_above_mean(arr_tmp).to_numpy(),\n",
    "                    count_below_mean(arr_tmp).to_numpy(),\n",
    "                    first_location_of_maximum(arr_tmp).to_numpy(),\n",
    "                    first_location_of_minimum(arr_tmp).to_numpy(),\n",
    "                    has_duplicates(arr_tmp).to_numpy(),\n",
    "                    has_duplicate_max(arr_tmp).to_numpy(),\n",
    "                    kurtosis(arr_tmp).to_numpy(),\n",
    "                    last_location_of_maximum(arr_tmp).to_numpy(),\n",
    "                    last_location_of_minimum(arr_tmp).to_numpy(),\n",
    "                    has_duplicate_min(arr_tmp).to_numpy(),\n",
    "                    longest_strike_above_mean(arr_tmp).to_numpy(),\n",
    "                    longest_strike_below_mean(arr_tmp).to_numpy(),\n",
    "                    maximum(arr_tmp).to_numpy(),\n",
    "                    mean_absolute_change(arr_tmp).to_numpy(),\n",
    "                    minimum(arr_tmp).to_numpy(),\n",
    "                    number_crossing_m(arr_tmp, 0).to_numpy(),\n",
    "                    mean(arr_tmp).to_numpy(),\n",
    "                    median(arr_tmp).to_numpy(),\n",
    "                    mean_change(arr_tmp).to_numpy(),\n",
    "                    ratio_value_number_to_time_series_length(arr_tmp).to_numpy(),\n",
    "                    skewness(arr_tmp).to_numpy(),\n",
    "                    standard_deviation(arr_tmp).to_numpy(),\n",
    "                    sum_of_reoccurring_values(arr_tmp).to_numpy(),\n",
    "                    sum_values(arr_tmp).to_numpy(),\n",
    "                    variance(arr_tmp).to_numpy(),\n",
    "                    variance_larger_than_standard_deviation(arr_tmp).to_numpy()\n",
    "                            ])\n",
    "# Prints the time taken in order to get 28 features.\n",
    "print(\"Time to extract the features : \" + str(time.time() - start) )\n",
    "# Let's transpose the Khiva array.\n",
    "features = features.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the Features matrix and the Targets matrix\n",
    "y = all_sites[\"SUB_INDUSTRY\"].values\n",
    "X = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesses the feature matrix.\n",
    "X = scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes several shuffles to distribute the samples.\n",
    "for i in range(15):\n",
    "    X, y, file_names = shuffle(X, y, file_names, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MY TEST VECTOR: ['Grocer/Market' 'Primary/Secondary School' 'Food Processing'\n",
      " 'Food Processing' 'Grocer/Market' 'Primary/Secondary School'\n",
      " 'Grocer/Market' 'Food Processing' 'Shopping Center/Shopping Mall'\n",
      " 'Shopping Center/Shopping Mall' 'Manufacturing'\n",
      " 'Primary/Secondary School' 'Shopping Center/Shopping Mall'\n",
      " 'Primary/Secondary School' 'Grocer/Market']\n",
      "MY PREDICTION: ['Grocer/Market' 'Primary/Secondary School' 'Food Processing'\n",
      " 'Food Processing' 'Grocer/Market' 'Primary/Secondary School'\n",
      " 'Grocer/Market' 'Food Processing' 'Shopping Center/Shopping Mall'\n",
      " 'Shopping Center/Shopping Mall' 'Food Processing'\n",
      " 'Primary/Secondary School' 'Primary/Secondary School' 'Food Processing'\n",
      " 'Grocer/Market']\n",
      "NUMBER OF ERRORS: 3\n",
      "ERROR RATE: 0.19999999999999996%\n",
      "ACCURACY: 0.8%\n",
      "PARAMETERS USED: {'degree': 3, 'probability': True, 'shrinking': True}\n"
     ]
    }
   ],
   "source": [
    "# Creates a model, fits it and predicts a subset of samples.\n",
    "files_test = []\n",
    "list_test_indices = []\n",
    "for i in range(len(file_names)):\n",
    "    if file_names[i] in [92, 45, 761, 10, 766, 400, 673, 49, 144, 496, 731, 281, 213, 197, 399]:\n",
    "        list_test_indices.append(i)\n",
    "        files_test.append(file_names[i])\n",
    "\n",
    "X_train = np.delete(X, list_test_indices, 0)\n",
    "X_test = np.take(X, list_test_indices, 0)\n",
    "y_train = np.delete(y, list_test_indices)\n",
    "y_test = np.take(y, list_test_indices)\n",
    "\n",
    "k_range_parameter = {'degree':[3,4],'shrinking':[True,False],'probability':[True,False]}\n",
    "\n",
    "# SVC is selected because of: >50 samples, predicting a category, labeled data and <100K samples. Good results.\n",
    "clf = svm.SVC()\n",
    "\n",
    "# Apply a gridSearchCV in order to get the best estimator during the training and cross-validation step.\n",
    "mygridsearch = GridSearchCV(clf, k_range_parameter, cv = 10, scoring = 'accuracy' )\n",
    "mygridsearch.fit(X_train, y_train)\n",
    "print(\"MY TEST VECTOR: \" + str(y_test))\n",
    "print(\"MY PREDICTION: \" + str(y_pred))\n",
    "print(\"NUMBER OF ERRORS: \" + str(sum(y_pred != y_test)))\n",
    "print(\"ERROR RATE: \" + str(1 - sum(y_pred == y_test) / float(len(y_pred))) + \"%\")\n",
    "print(\"ACCURACY: \" + str(sum(y_pred == y_test) / float(len(y_pred))) + \"%\")\n",
    "print(\"PARAMETERS USED: \"+ str(mygridsearch.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
